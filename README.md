# High-Level Overview of File Metadata and Download Records Information: Sage Bionetworks Alzheimer's Disease (AD) Knowledge Portal

[![Python logo](./python_logo.JPG)](https://www.python.org/)

### To learn more about Sage Bionetworks, [please click here.](https://sagebionetworks.org/)
### To learn more about the AD Knowledge Portal, [please click here.](https://adknowledgeportal.synapse.org/)

[![Sage Bionetworks logo](./sage_bionetworks_logo.png)](https://sagebionetworks.org/)

IMPORTANT:

* Some language in this READ_ME documentation is similar to that for a Python script produced during my work at [Michigan Publishing](https://publishing.umich.edu/), Fall 2020.

* Please visit [this page](https://github.com/stlouiss/ACLS_Humanities_eBook_Collection_Metadata_Curation) if you would like to learn more about that Michigan Publishing script.

* REQUESTS FOR INPUT AND/OR OUTPUT DATA will be considered on a case-by-case basis by professional contacts at Sage Bionetworks. Please submit such requests to the email address listed at the bottom of this READ_ME file.


## IN SUMMARY:

This program pulls in values from select fields of two input CSV files, processes this information with a number of functions divided into Sections 1-3, and writes high-level overview information into three separate output CSV files.

The output CSV files of this script served simply as springboards to more contextually informed conversations with stakeholders at Sage Bionetworks, drawing on their expert knowledge of the AD Knowledge Portal and the research communities it serves. It is in these conversations that we established key considerations regarding data context and inter-relationality to inform the scope of my preservation assessment research.

**As such, I make no claim that any of the script outputs surfaced the most important qualities of the AD Knowledge Portal for preservation assessment purposes.**

*Indeed, many dimensions of the AD Knowledge Portal -- indispensable to the facilitation of potential future data reuse -- are either dramatically understated or made virtually invisible by the output files of this script. Simply stated, the output files have extremely LIMITED descriptive/explanatory potential!*

In other words, the output CSV files generated by this script served *only* the purpose of launching much richer conversations with data management experts at Sage Bionetworks about AD Knowledge Portal context and content. Such conversations were essential to informed preservation assessment research, while the script outputs in and of themselves were not.

Starting with a high-level overview -- **perhaps even a superficial one** -- galvanized extremely helpful discussions about the most important aspects of the AD Knowledge Portal for long-term preservation purposes, in the spirit of facilitating potential future data reuse. 

In order to make explicit the contexts, practices, and priorities that should inform my preservation assessment research, it was (counterintuitively) helpful to begin with aids to discussion -- the script's output files -- that likely *erased* such qualities. Such erasure promoted stakeholder responses that made explicit a great deal of important information, well-known to Sage Bionetworks staff, for the benefit of a graduate-student preservation consultant (myself), heretofore unfamiliar with the client organization and the world of Alzheimer's Disease research data more broadly. 

Hence, my characterization of the script output files as "springboards to more contextually informed conversations" will begin to make more sense. **By launching certain stakeholder conversations with a simple question -- "What do I need to know about AD Knowledge Portal data that these output files do not make clear?" -- I set the stage for extremely rich interactions that surfaced key information which might otherwise have remained implicit,** due to the common experiences and understanding among Sage Bionetworks staff that I did share. After all, I was a graduate student external to the organization (with a different academic background, to boot).


## Python Script for High-Level Overview of File and Download Information

* Input CSV files: 
  * "ad_knowledge_portal_files_information.csv" (file metadata CSV file)
  * "ad_knowledge_portal_downloads_june_december_2020.csv" (download information CSV file, June-December 2020)
   * Input CSV files went in the same directory on my machine as the Python code. Doing so was necessary for the script to run correctly.
   * Tnput CSV files contained 99764 and 205133 rows respectively, excluding the header rows. 
   * Given the size of the input CSV files, this program took several minutes to run. Patience is key.

* Final output CSV files: 
  * "output_file_info_values_and_counts.csv"
  * "output_download_info_values_and_counts.csv"
  * "output_study_info_file_and_download_values_and_count"
   * These output CSV files required some light cleaning in a spreadsheet application for presentation purposes.
   * The output CSV files of this script served simply as a springboard to a more contextually informed conversation with stakeholders at Sage Bionetworks, drawing on their expert knowledge of the AD Knowledge Portal and the research communities it serves. It is in this conversation that I established priorities with which to scope my preservation research.

* One manually created Excel spreadsheet:
  * "downloadinfo_to_filemetadata_identifier_columns_comparison.xlsx"
   * This Excel spreadsheet determined the number of file identifiers in the download information CSV file that were not in the file metadata CSV file. Determining this value by sleuthing in Excel was necessary for confirming that Section 2 of the Python script was running correctly. See the output information for Section 2 in the "execution" component of the Python code (near the bottom of the code) to learn more.
   * The following tutorial was helpful in developing this Excel file: ["Check If One Column Value Exists in Another Column,"](https://www.got-it.ai/solutions/excel-chat/excel-tutorial/vlookup/check-if-one-column-value-exists-in-another-column) (accessed March 22, 2021).


## What does the Python script do? (SECTION 1)

The main goal of Section 1 in the Python script is to return a CSV file listing, in descending order of frequency, the file format information offered by two different columns in the input spreadsheet: "fileFormat" and "name" (the second column includes format-extension values in the file names).

## What does the Python script do? (SECTION 2)

The main goal of Section 2 in the Python script is to return a CSV file listing, in descending order of frequency, download information organized by file format and filename format extension value.

## What does the Python script do? (SECTION 3)

The main goal of Section 3 in the Python script is to return a CSV file listing, in descending order of frequency, the number of file downloads and the total number of files in the AD Knowledge Portal organized by study.
    
    
## Functions: Section 1 (Managing File Information Metadata)


### read_file_info_csv_with_pandas(filename)
Opens the input CSV file into a DataFrame object, drops all columns except those specified as output headers, and converts the DataFrame into a list of row lists that will be used in later functions detailed below. 

This function was first written by Joe Muller for a [project at Michigan Publishing](https://github.com/stlouiss/ACLS_Humanities_eBook_Collection_Metadata_Curation), during a troubleshooting session with Scott St. Louis, November 2020.

This specific function is now being put to use here to determine the frequency of various file formats in the Sage Bionetworks AD Knowledge Portal.

Returns list_of_row_lists_file_info.


### create_fileFormat_list(list_of_row_lists_file_info)
Creates a list of values from the "fileFormat" column in the input spreadsheet by pulling these values out of the list of row lists generated by the read_csv_with_pandas function.

Returns fileFormat_list.


### create_fileName_list(list_of_row_lists_file_info)
Creates a list of values from the "name" column in the input spreadsheet by pulling values out of the list of row lists generated by the read_csv_with_pandas function.

Returns fileName_list.


### count_fileFormats(fileFormat_list)
Iterates through the list of file format values to generate a descending-order count of the various file formats in the AD Knowledge Portal.

Returns sorted_fileFormat_info.


### count_fileNameExtensions(fileName_list)
Pulls format-extension values out of file names and into a new list, then iterates through the new list to generate a descending-order count of the various format-extension values in the AD Knowledge Portal file names.
  
Returns sorted_fileNameExtensions_info.


### write_preliminary_data_processing_results(output_filename, sorted_fileFormat_info, sorted_fileNameExtensions_info)
Writes the descending-order counts generated by the previous two functions into a new CSV spreadsheet. 

This output CSV might require some light styling in a spreadsheet application for presentation purposes.

Returns output_csv.


## Functions: Section 2 (Managing Download Information, June-Dec 2020)


### read_download_info_csv_with_pandas(filename)
Opens the input CSV file into a DataFrame object, drops all columns except those specified as output headers, and converts the DataFrame into a list of row lists that will be used in later functions detailed below. 

This function was first written by Joe Muller for a [project at Michigan Publishing](https://github.com/stlouiss/ACLS_Humanities_eBook_Collection_Metadata_Curation), during a troubleshooting session with Scott St. Louis, November 2020.

This specific function is now being put to use here to determine the frequency of downloads for various file formats in the Sage Bionetworks AD Knowledge Portal from June to December 2020.

Returns list_of_row_lists_download_info.


### download_frequency_by_identifier(list_of_row_lists_download_info)
Counts unique file identifiers from download records in the June-December 2020 AD Knowledge Portal download information CSV, and returns a descending-order dictionary of unique file identifiers by number of downloads.

Returns sorted_file_identifier_download_info.


### create_intersection_list(list_of_row_lists_file_info, list_of_row_lists_download_info)
Creates a list of file identifiers located in both input spreadsheets, along with the file format corresponding to each file identifier, for later functions to process.

Returns intersection_list_with_file_format_info.


### download_frequency_by_file_format(intersection_list_with_file_format_info, sorted_file_identifier_download_info)
Counts download frequency by file format for files listed in the June-December 2020 AD Knowledge Portal download information CSV, and returns a descending-order dictionary of download counts organized by file format.

Returns sorted_file_format_info.


### download_frequency_by_filename_format_extension(list_of_row_lists_download_info)
Utilizes download information CSV to produce a descending-order list of filename format extension values by download frequency, June-December 2020.

Returns sorted_filename_format_extensions_info.


### write_download_info_to_csv_file(output_filename, sorted_file_format_info, sorted_filename_format_extensions_info)
Writes values and counts for download information into an output CSV.

Returns output_csv_2.


## Functions: Section 3 (Organizing File and Download Information by Study)


### download_count_by_study(list_of_row_lists_download_info)
Counts number of file downloads by study.

Returns sorted_study_download_info.


### file_count_by_study(list_of_row_lists_file_info)
Counts number of files in AD Knowledge Portal by study.

Returns sorted_file_count_info_by_study.


### write_study_info_to_csv_file(output_filename, sorted_study_download_info, sorted_file_count_info_by_study)
Writes values and counts for file and download information organized by study into an output CSV.

Returns output_csv_3.


### Acknowledgments

Special thanks to Erin Annis, Danah Bazzi, and Rachel Gosch for all of their helpful feedback in our regular study group as classmates pursuing independent projects for SI 699.

Even more gratitude goes to Rachel Gosch for helping me get familiar with the process of writing detailed Python READ_ME documentation during the [aforementioned Michigan Publishing project.](https://github.com/stlouiss/ACLS_Humanities_eBook_Collection_Metadata_Curation)

Thanks also to Professor Pasquetto and the UMSI Engaged Learning Office for providing me with the opportunity to work on this important project, as well as to everyone at Sage Bionetworks for your extremely valuable and inspiring contributions to science.

### Questions?

Get in touch via email! 

You can contact the developer here:

* stlouiss [AT] umich [DOT] edu


### Thank you!

* Scott St. Louis (code developer)


### SI 699 (Advanced Digital Curation)
### [University of Michigan School of Information (UMSI)](https://www.si.umich.edu/)
### March 2021
